{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13059697,"sourceType":"datasetVersion","datasetId":8270117},{"sourceId":166258,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141469,"modelId":164048}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\n# 基础模型路径\nbase_model_path = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1\"\n# LoRA 适配器路径\nlora_adapter_path = \"/kaggle/input/opendeepdiver-sft-lora/qwen2.5-7b-lora-adapter\"\n# 合并后模型的保存路径\nsave_path = \"/kaggle/working/merged_model\"\n\n# 加载基础模型和分词器\nprint(\"加载基础模型...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,  # 根据需求调整 dtype（如 float32）\n    device_map=\"auto\",\n    trust_remote_code=True  # 如果 Qwen 需要此参数\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n\n# 加载 LoRA 适配器\nprint(\"加载 LoRA 适配器...\")\nmodel = PeftModel.from_pretrained(model, lora_adapter_path)\n\n# 合并模型（将 LoRA 权重合并到基础模型中）\nprint(\"合并 LoRA 适配器...\")\nmodel = model.merge_and_unload()\n\n# 保存合并后的模型\nprint(f\"保存合并后的模型到 {save_path}...\")\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(\"合并完成！\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}