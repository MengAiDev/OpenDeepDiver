{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13047432,"sourceType":"datasetVersion","datasetId":8262061},{"sourceId":166258,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141469,"modelId":164048}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mengaidev/open-deep-diver-sft?scriptVersionId=261757714\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nos.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n!pip install --upgrade -qqq uv\ntry: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\nexcept: get_numpy = \"numpy\"\ntry: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\nexcept: is_t4 = False\nget_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n!uv pip install -qqq --upgrade     unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n!uv pip install -qqq {get_triton}\n!uv pip install \"huggingface_hub>=0.34.0\" \"datasets>=3.4.1,<4.0.\n!uv pip install transformers==4.55.4\n!uv pip install --no-deps trl==0.22.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom datasets import Dataset\nimport json\n\n# 加载 Qwen2.5-7B 4bit 量化模型\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"/kaggle/input/qwen2.5/transformers/7b-instruct/1\",\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n)\n\n# 添加 LoRA 适配器\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA 秩\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# 加载和处理数据集\ndef load_data(file_path):\n    data = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip():\n                item = json.loads(line)\n                \n                # 根据是否有 context 字段构建不同的输入格式\n                if \"context\" in item and item[\"context\"]:\n                    input_text = f\"上下文: {item['context']}\\n问题: {item['question']}\"\n                else:\n                    input_text = f\"问题: {item['question']}\"\n                \n                data.append({\n                    \"input\": input_text,\n                    \"output\": item[\"answer\"],\n                })\n    return Dataset.from_list(data)\n\n# 加载数据集\ndataset = load_data(\"/kaggle/input/openwebpuzzle/webpuzzle_dataset.jsonl\") \n\n# 格式化函数 - 使用 Qwen2.5 的聊天模板\ndef formatting_prompts(examples):\n    texts = []\n    for input_text, output in zip(examples[\"input\"], examples[\"output\"]):\n        # 使用 Qwen2.5 的聊天格式\n        messages = [\n            {\"role\": \"user\", \"content\": input_text},\n            {\"role\": \"assistant\", \"content\": output}\n        ]\n        text = tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=False\n        )\n        texts.append(text)\n    return {\"text\": texts}\n\n# 应用格式化\ndataset = dataset.map(formatting_prompts, batched=True)\n\n# 训练参数 - 设置为 3 个训练周期\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen2.5-7b-sft-output\",\n    num_train_epochs=3,  # 3 个训练周期\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    report_to=[]\n)\n\n# 创建训练器\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=1024,\n    args=training_args,\n    packing=False,  # 不打包序列以提高训练效率\n)\n\n# 开始训练!\ntrainer.train()\n\n# 保存模型\nmodel.save_pretrained(\"qwen2.5-7b-lora-adapter\")  # 保存 LoRA 适配器\ntokenizer.save_pretrained(\"qwen2.5-7b-lora-adapter\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}